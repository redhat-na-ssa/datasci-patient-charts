{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5498e4f6-9129-4efa-84dd-9d77220635bc",
   "metadata": {},
   "source": [
    "# 1 - Normalize Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91844a6-62c2-4483-bc48-b3b717f0648f",
   "metadata": {},
   "source": [
    "This Lab material is an adaptation of the course 'Preprocessing Unstructured Data for LLM Applications', Coursera, March 2024\n",
    "\n",
    "In this jupyter notebook we will learn how to normalize pdf content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a3cc98-7de7-4d33-98fa-dcc1bf04eb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured-client\n",
      "  Downloading unstructured_client-0.27.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio>=1.6.0 in /opt/app-root/lib/python3.9/site-packages (from unstructured-client) (1.6.0)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Collecting python-dateutil==2.8.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pypdf>=4.0\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m201.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=3.1 in /opt/app-root/lib/python3.9/site-packages (from unstructured-client) (40.0.2)\n",
      "Collecting pydantic<2.10.0,>=2.9.2\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m240.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<0.10.0,>=0.9.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting httpx>=0.27.0\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m172.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting eval-type-backport<0.3.0,>=0.2.0\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil==2.8.2->unstructured-client) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/app-root/lib/python3.9/site-packages (from cryptography>=3.1->unstructured-client) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.9/site-packages (from httpx>=0.27.0->unstructured-client) (2024.7.4)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib/python3.9/site-packages (from httpx>=0.27.0->unstructured-client) (1.3.1)\n",
      "Requirement already satisfied: anyio in /opt/app-root/lib/python3.9/site-packages (from httpx>=0.27.0->unstructured-client) (4.4.0)\n",
      "Requirement already satisfied: idna in /opt/app-root/lib/python3.9/site-packages (from httpx>=0.27.0->unstructured-client) (3.8)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m235.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m197.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.23.4\n",
      "  Downloading pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m135.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/app-root/lib/python3.9/site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.1 in /opt/app-root/lib/python3.9/site-packages (from requests-toolbelt>=1.0.0->unstructured-client) (2.32.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from typing-inspect<0.10.0,>=0.9.0->unstructured-client) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/app-root/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client) (2.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=1.0.0->unstructured-client) (1.26.19)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=1.0.0->unstructured-client) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/app-root/lib/python3.9/site-packages (from anyio->httpx>=0.27.0->unstructured-client) (1.2.2)\n",
      "Installing collected packages: typing-inspect, python-dateutil, pypdf, pydantic-core, jsonpath-python, h11, eval-type-backport, annotated-types, requests-toolbelt, pydantic, httpcore, httpx, unstructured-client\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 0.10.1\n",
      "    Uninstalling requests-toolbelt-0.10.1:\n",
      "      Successfully uninstalled requests-toolbelt-0.10.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.18\n",
      "    Uninstalling pydantic-1.10.18:\n",
      "      Successfully uninstalled pydantic-1.10.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.8.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "codeflare-sdk 0.19.1 requires pydantic<2, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 eval-type-backport-0.2.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpath-python-1.0.6 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-5.1.0 python-dateutil-2.8.2 requests-toolbelt-1.0.0 typing-inspect-0.9.0 unstructured-client-0.27.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install unstructured_client    #from old SDK\n",
    "!pip install unstructured-client     #new SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ec6839-a1fe-4eaf-93ab-4ffa3ee585cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "import unstructured_client\n",
    "from unstructured_client.models import shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109dd70-bef4-4656-8b88-846ce03e11a0",
   "metadata": {},
   "source": [
    "## Examine PDF Files\n",
    "\n",
    "In the **datasci-patient-charts/example_files** directory, double click on each pdf to see what example data the team has to work with.\n",
    "Closely examine the file **CP_CHRT_C_G4M3BA_De-identified.pdf**  In this lab we will work with this pdf. \n",
    "\n",
    "PDFs are different than processing HTML or ppts.  Where in those documents you are looking at semi-structured information for clues on how to divide element types within the documents. In pdfs you are going to look for things like formatting.  For example:\n",
    "* a piece of text that is 'bolded' or 'underlined' may more likely be a title.\n",
    "* text that is longer and blockier, contains multiple sentences, doesn't have emphasis (e.g. bolding, underlining) is more likely to be narrative text.\n",
    "\n",
    "Let's take the example medical file (CP_CHRT_C_G4M3BA_De-identified.pdf) and pass it to the unstructured API where the unstructured.io model is setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a089fd-9fb5-48a5-bd61-597c50368751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.unstructured.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Image', 'element_id': 'df79fa92715475e38b3320154fa85207', 'text': 'PAST MEDICAL HISTORY ', 'metadata': {'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'filename': 'CP_CHRT_C_G4M3BA_De-identified.pdf'}}\n"
     ]
    }
   ],
   "source": [
    "#Use your stored keys from the healthcare workbench\n",
    "client = unstructured_client.UnstructuredClient(\n",
    "    api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "    server_url=os.getenv(\"UNSTRUCTURED_API_URL\"),\n",
    ")\n",
    "\n",
    "filename = \"example_files/CP_CHRT_C_G4M3BA_De-identified.pdf\"\n",
    "filename_out = \"example_files_out/json_elements.json\"\n",
    "\n",
    "req = {\n",
    "    \"partition_parameters\": {\n",
    "        \"files\": {\n",
    "            \"content\": open(filename, \"rb\"),\n",
    "            \"file_name\": filename,\n",
    "        },\n",
    "        \"strategy\": shared.Strategy.HI_RES,\n",
    "        \"languages\": ['eng'],\n",
    "        \"split_pdf_page\": True,            # If True, splits the PDF file into smaller chunks of pages.\n",
    "        \"split_pdf_allow_failed\": True,    # If True, the partitioning continues even if some pages fail.\n",
    "        \"split_pdf_concurrency_level\": 15  # Set the number of concurrent request to the maximum value: 15.\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    res = client.general.partition(request=req)\n",
    "    element_dicts = [element for element in res.elements]\n",
    "\n",
    "    # Print the processed data's first element only.\n",
    "    print(element_dicts[0])\n",
    "\n",
    "    # Write the processed data to a local file.\n",
    "    json_elements = json.dumps(element_dicts, indent=2)\n",
    "\n",
    "    with open(filename_out, \"w\") as file:\n",
    "        file.write(json_elements)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02697ea-f51f-4878-b6d5-53829690bf78",
   "metadata": {},
   "source": [
    "\n",
    "Examine the original PDF example_files/**CP_CHRT_C_G4M3BA_De-identified.pdf** and the resulting **json_elements.json** file.\n",
    "\n",
    "Compare the document types (and associated text). Do they match the document types (e.g. Title, Narrative text, List items) within the original PDF?\n",
    "\n",
    "Note: that you are able to visually identify that 'SURGICAL HISTORY' is a title. And it would get the same normalized type as a title serialized from a ppt or HTML file.\n",
    "\n",
    "Also note the 'element_id' that is created and associated with each text type.\n",
    "\n",
    "Finally, look at the 'metadata' tag. If you expand it, you will see the 'name' and the 'page number' of the document that this structured data was produced from.\n",
    "\n",
    "Now that we were able to obtain document elements and metadata, we are ready to perform metadata extraction and chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757a9c6-4a9d-4fe7-bc3a-75a28fa4ba00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
